# 
msgid ""
msgstr ""
"Project-Id-Version: OpenNebula 4.6\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2014-04-30 12:52+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../source/administration/storage/ceph_ds.rst:5
msgid "The Ceph Datastore"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:7
msgid ""
"The Ceph datastore driver provides OpenNebula users with the possibility of "
"using Ceph block devices as their Virtual Images."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:9
msgid ""
"This driver **only** works with libvirt/KVM drivers. Xen is not (yet) "
"supported."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:11
msgid ""
"This driver requires that the OpenNebula nodes using the Ceph driver must be"
" part of a running Ceph cluster. More information in `Ceph documentation "
"<http://ceph.com/docs/master/>`__."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:13
msgid ""
"The hypervisor nodes need to be part of a working Ceph cluster and the "
"Libvirt and QEMU packages need to be recent enough to have support for Ceph."
" For Ubuntu systems this is available out of the box, however for CentOS "
"systems you will need to manually install `this version "
"<http://ceph.com/packages/qemu-kvm/>`__ of qemu-kvm."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:16
msgid "Requirements"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:19
msgid "Ceph Cluster Configuration"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:21
msgid ""
"The hosts where Ceph datastores based images will be deployed must be part "
"of a running Ceph cluster. To do so refer to the `Ceph documentation "
"<http://ceph.com/docs/master/>`__."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:23
msgid ""
"The Ceph cluster must be configured in such a way that no specific "
"authentication is required, which means that for ``cephx`` authentication "
"the keyring must be in the expected path so that ``rbd`` and ``ceph`` "
"commands work without specifying explicitely the keyring's location."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:25
msgid ""
"Also the ``mon`` daemon must be defined in the ``ceph.conf`` for all the "
"nodes, so ``hostname`` and ``port`` doesn't need to be specified explicitely"
" in any Ceph command."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:27
msgid ""
"Additionally each OpenNebula datastore is backed by a ceph pool, these pools"
" must be created and configured in the Ceph cluster. The name of the pool by"
" default is ``one`` but can be changed on a per-datastore basis (see below)."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:29
msgid ""
"This driver can work with either RBD Format 1 or RBD Format 2. To set the "
"default you can specify this option in ``ceph.conf``:"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:38
msgid "OpenNebula Ceph Frontend"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:40
msgid ""
"This driver requires the system administrator to specify one or several Ceph"
" frontends (which need to be nodes in the Ceph cluster) where many of the "
"datastores storage actions will take place. For instance, when creating an "
"image, OpenNebula will choose one of the listed Ceph frontends (using a "
"round-robin algorithm) and transfer the image to that node and run ``qemu-"
"img convert -O rbd``. These nodes need to be specified in the "
"``BRIDGE_LIST`` section."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:42
msgid ""
"Note that this Ceph frontend can be any node in the OpenNebula setup: the "
"OpenNebula frontend, any worker node, or a specific node (recommended)."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:45
msgid "Ceph Nodes"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:47
msgid ""
"All the nodes listed in the ``BRIDGE_LIST`` variable must have\\ ``qemu-"
"img`` installed."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:50
msgid "OpenNebula Hosts"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:52
msgid ""
"There are no specific requirements for the host, besides being libvirt/kvm "
"nodes, since xen is not (yet) supported for the Ceph drivers."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:55
msgid "Configuration"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:58
msgid "Configuring the System Datastore"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:60
msgid ""
"To use ceph drivers, the system datastore will work both with ``shared`` or "
"as ``ssh``. This sytem datastore will hold only the symbolic links to the "
"block devices, so it will not take much space. See more details on the "
":ref:`System Datastore Guide <system_ds>`"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:62
msgid ""
"It will also be used to hold context images and Disks created on the fly, "
"they will be created as regular files."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:65
msgid "Configuring Ceph Datastores"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:67
msgid ""
"The first step to create a Ceph datastore is to set up a template file for "
"it. In the following table you can see the supported configuration "
"attributes. The datastore type is set by its drivers, in this case be sure "
"to add ``DS_MAD=ceph`` and ``TM_MAD=ceph`` for the transfer mechanism, see "
"below."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:70
msgid "Attribute"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:70
msgid "Description"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:72
msgid "``NAME``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:72
msgid "The name of the datastore"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:74
msgid "``DS_MAD``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:74
msgid "The DS type, use ``ceph`` for the Ceph datastore"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:76
msgid "``TM_MAD``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:76
msgid "Transfer drivers for the datastore, use ``ceph``, see below"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:78
msgid "``DISK_TYPE``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:78
msgid "The type **must** be ``RBD``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:80
msgid "``BRIDGE_LIST``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:80
msgid ""
"**Mandatory** space separated list of Ceph servers that are going to be used"
" as frontends."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:82
msgid "``POOL_NAME``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:82
msgid ""
"The OpenNebula Ceph pool name. Defaults to ``one``. **This pool must exist "
"before using the drivers**."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:84
msgid "``STAGING_DIR``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:84
msgid "Default path for image operations in the OpenNebula Ceph frontend."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:86
msgid "``RESTRICTED_DIRS``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:86
msgid ""
"Paths that can not be used to register images. A space separated list of "
"paths."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:88
msgid "``SAFE_DIRS``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:88
msgid ""
"If you need to un-block a directory under one of the RESTRICTED\\_DIRS. A "
"space separated list of paths."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:90
msgid "``NO_DECOMPRESS``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:90
msgid ""
"Do not try to untar or decompress the file to be registered. Useful for "
"specialized Transfer Managers"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:92
msgid "``LIMIT_TRANSFER_BW``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:92
msgid ""
"Specify the maximum transfer rate in bytes/second when downloading images "
"from a http/https URL. Suffixes K, M or G can be used."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:94
msgid "``DATASTORE_CAPACITY_CHECK``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:94
msgid ""
"If ``yes``, the available capacity of the datastore is checked before "
"creating a new image"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:96
msgid "``CEPH_HOST``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:96
msgid ""
"Space-separated list of Ceph monitors. Example: ``host1 host2:port2 host3 "
"host4:port4`` (if no port is specified, the default one is chosen). "
"**Required for Libvirt 1.x when cephx is enabled** ."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:98
msgid "``CEPH_SECRET``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:98
msgid ""
"A generated UUID for a LibVirt secret (to hold the CephX authentication key "
"in Libvirt on each hypervisor). This should be generated when creating the "
"Ceph datastore in OpenNebula. **Required for Libvirt 1.x when cephx is "
"enabled** ."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:100
msgid "``RBD_FORMAT``"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:100
msgid ""
"By default RBD Format 1 will be used, with no snapshotting support. If "
"``RBD_FORMAT=2`` is specified then when instantiating non-persistent images "
"the Ceph driver will perform ``rbd snap`` instead of ``rbd copy``."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:103
msgid ""
"This will prevent users registering important files as VM images and "
"accessing them through their VMs. OpenNebula will automatically add its "
"configuration directories: /var/lib/one, /etc/one and oneadmin's home. If "
"users try to register an image from a restricted directory, they will get "
"the following error message: “Not allowed to copy image file”."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:105
msgid ""
"For example, the following examples illustrates the creation of an Ceph "
"datastore using a configuration file. In this case we will use the host "
"``cephfrontend`` as one the OpenNebula Ceph frontend The ``one`` pool must "
"already exist, if it doesn't create it with:"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:114
msgid "An example of datastore:"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:138
msgid ""
"The DS and TM MAD can be changed later using the ``onedatastore update`` "
"command. You can check more details of the datastore by issuing the "
"``onedatastore show`` command."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:140
msgid ""
"Note that datastores are not associated to any cluster by default, and they "
"are supposed to be accessible by every single host. If you need to configure"
" datastores for just a subset of the hosts take a look to the :ref:`Cluster "
"guide <cluster_guide>`."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:142
msgid ""
"After creating a new datastore the LN\\_TARGET and CLONE\\_TARGET parameters"
" will be added to the template. These values should not be changed since "
"they define the datastore behaviour. The default values for these parameters"
" are defined in :ref:`oned.conf <oned_conf_transfer_driver>` for each "
"driver."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:145
msgid "Using Datablocks with Ceph"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:147
msgid ""
"It is worth noting that when creating datablock, creating a RAW image is "
"very fast whereas creating a formatted block device takes a longer time. If "
"you want to use a RAW image remember to use the following attribute/option "
"when creating the Image datablock: ``FS_TYPE = RAW``."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:150
msgid "Ceph Authentication (Cephx)"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:152
msgid ""
"If `Cephx <http://ceph.com/docs/master/rados/operations/authentication/>`__ "
"is enabled, there are some special considerations the OpenNebula "
"administrator must take into account."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:154
msgid ""
"Create a Ceph user for the OpenNebula hosts. We will use the name "
"``client.libvirt``, but any other name is fine. Create the user in Ceph and "
"grant it rwx permissions on the ``one`` pool:"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:160
msgid ""
"Extract the ``client.libvirt`` key, save it to a file named "
"``client.libvirt.key``\\ and distribute it to all the KVM hosts:"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:167
msgid ""
"Generate a UUID, for example running ``uuigden`` (the generated uuid will "
"referenced as ``%UUID%`` from now onwards)."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:169
msgid ""
"Create a file named ``secret.xml`` (using the genereated ``%UUID%`` and "
"distribute it to all the KVM hosts:"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:182
msgid ""
"The following commands must be executed in all the KVM hosts as oneadmin "
"(assuming the ``secret.xml`` and ``client.libvirt.key`` files have been "
"distributed to the hosts):"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:189
msgid ""
"Finally, the Ceph datastore must be updated to add the following values:"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:197
msgid ""
"You can read more information about this in the Ceph guide `Using libvirt "
"with Ceph <http://ceph.com/docs/master/rbd/libvirt/>`__."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:200
msgid "Using the Ceph Transfer Driver"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:202
msgid ""
"The workflow for Ceph images is similar to the other datastores, which means"
" that a user will create an image inside the Ceph datastores by providing a "
"path to the image file locally available in the OpenNebula frontend, or to "
"an http url, and the driver will convert it to a Ceph block device."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:204
msgid ""
"All the usual operations are avalaible: oneimage create, oneimage delete, "
"oneimage clone, oneimage persistent, oneimage nonpersistent, onevm disk-"
"snapshot, etc..."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:207
msgid "Tuning & Extending"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:210
msgid "File Location"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:212
msgid ""
"System administrators and integrators are encouraged to modify these drivers"
" in order to integrate them with their datacenter:"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:214
msgid "Under ``/var/lib/one/remotes/``:"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:216
msgid "**datastore/ceph/ceph.conf**: Default values for ceph parameters"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:218
msgid "HOST: Default OpenNebula Ceph frontend"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:219
msgid "POOL\\_NAME: Default volume group"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:220
msgid ""
"STAGING\\_DIR: Default path for image operations in the OpenNebula Ceph "
"frontend."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:222
msgid ""
"**datastore/ceph/cp**: Registers a new image. Creates a new logical volume "
"in ceph."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:223
msgid ""
"**datastore/ceph/mkfs**: Makes a new empty image. Creates a new logical "
"volume in ceph."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:224
msgid "**datastore/ceph/rm**: Removes the ceph logical volume."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:225
msgid "**tm/ceph/ln**: Does nothing since it's handled by libvirt."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:226
msgid "**tm/ceph/clone**: Copies the image to a new image."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:227
msgid "**tm/ceph/mvds**: Saves the image in a Ceph block device for SAVE\\_AS."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:228
msgid ""
"**tm/ceph/delete**: Removes a non-persistent image from the Virtual Machine "
"directory if it hasn't been subject to a ``disk-snapshot`` operation."
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:231
msgid "Using SSH System Datastore"
msgstr ""

#: ../../source/administration/storage/ceph_ds.rst:233
msgid ""
"Another option would be to manually patch the post and pre-migrate scripts "
"for the **ssh** system datastore to ``scp`` the files residing in the system"
" datastore before the live-migration. `Read more "
"<http://lists.opennebula.org/pipermail/users-"
"opennebula.org/2013-April/022705.html>`__."
msgstr ""
